{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation,TimeDistributed\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate,BatchNormalization\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, RepeatVector, Permute, merge\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import *\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google word2vec\n",
    "from gensim.models import KeyedVectors as wv\n",
    "word_vectors1 = wv.load_word2vec_format('./input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "EMBEDDING_DIM =300\n",
    "\n",
    "#glove\n",
    "word_vectors2 = {}\n",
    "f = open('./input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    word_vectors2[word] = coefs\n",
    "f.close()\n",
    "EMBEDDING_DIM =300\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1044897, 3)\n",
      "Test shape :  (261225, 3)\n",
      "Train shape :  (387192, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133472 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# df_train = pd.read_csv(\"./input/train.csv\")\n",
    "# df_test = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "df = pd.read_csv(\"./input/train.csv\")\n",
    "df_train, df_test = train_test_split(df, test_size=0.2,random_state=1)\n",
    "print(\"Train shape : \",df_train.shape)\n",
    "print(\"Test shape : \",df_test.shape)\n",
    "\n",
    "df_train_pos = df_train[df_train['target']==1]\n",
    "df_train_neg = df_train[df_train['target']==0].sample(len(df_train_pos)*5,random_state=1)\n",
    "df_train = pd.concat([df_train_pos,df_train_neg])\n",
    "df_train = df_train.sample(frac=1,random_state=1)\n",
    "print(\"Train shape : \",df_train.shape)\n",
    "\n",
    "df_train.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=180\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(df_train['question_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df_train['question_text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "train_X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "train_Y = df_train['target']\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df_test['question_text'])\n",
    "test_X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "test_Y = df_test['target']\n",
    "# labels = to_categorical(np.asarray(labels))\n",
    "# print('Shape of data tensor:', data.shape)\n",
    "# print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "# indices = np.arange(data.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# data = data[indices]\n",
    "# labels = labels[indices]\n",
    "# nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "# x_train = data[:-nb_validation_samples]\n",
    "# y_train = labels[:-nb_validation_samples]\n",
    "# x_val = data[-nb_validation_samples:]\n",
    "# y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What can make you effective teacher?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train.iloc[4]['question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.65645984421165\n",
      "62.0\n",
      "41.84467927436718\n",
      "1017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3.15713e+05, 6.37790e+04, 7.69500e+03, 3.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00]),\n",
       " array([1.000e+00, 1.026e+02, 2.042e+02, 3.058e+02, 4.074e+02, 5.090e+02,\n",
       "        6.106e+02, 7.122e+02, 8.138e+02, 9.154e+02, 1.017e+03]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = []\n",
    "for sent in df_train['question_text']:\n",
    "#     print(sent)\n",
    "    lengths.append(len(sent))\n",
    "#     if max(sent)>50:\n",
    "#         print(sent)\n",
    "print(np.mean(lengths))\n",
    "print(np.median(lengths))\n",
    "print(np.std(lengths))\n",
    "print(np.max(lengths))\n",
    "plt.hist(lengths,density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for google word2vec\n",
    "\n",
    "embedding_matrix1 = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        word_vector1 = word_vectors1[word]\n",
    "#     if word_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix1[i] = word_vector1\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "embedding_matrix2 = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        word_vector2 = word_vectors2[word]\n",
    "#     if word_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix2[i] = word_vector2\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387192, 180)\n",
      "(387192,)\n"
     ]
    }
   ],
   "source": [
    "# print(train_X_raw[0])\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "# print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    # So we only measure F1 on the target y value:\n",
    "    y_true = y_true[:, 0]\n",
    "    y_pred = y_pred[:, 0]\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM+Attention  - Single embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 180, 300)          40041900  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 180, 300)          542400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 180, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               321600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 40,926,101\n",
      "Trainable params: 884,201\n",
      "Non-trainable params: 40,041,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix1],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "\n",
    "\n",
    "# model.add(CuDNNLSTM(75, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Bidirectional(CuDNNLSTM(150, return_sequences=True), input_shape=(train_X.shape[1], EMBEDDING_DIM)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Bidirectional(CuDNNLSTM(100, return_sequences=False)))#, input_shape=(train_X.shape[1], EMBEDDING_DIM)))\n",
    "# model.add(Attention(MAX_SEQUENCE_LENGTH))\n",
    "# model.add(Flatten())\n",
    "# model.add(TimeDistributed(Dense(100,activation='relu')))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',f1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348472 samples, validate on 38720 samples\n",
      "Epoch 1/10\n",
      "348472/348472 [==============================] - 161s 462us/step - loss: 0.2159 - acc: 0.9130 - f1: 0.7214 - val_loss: 0.1860 - val_acc: 0.9260 - val_f1: 0.7646\n",
      "Epoch 2/10\n",
      "348472/348472 [==============================] - 163s 469us/step - loss: 0.1846 - acc: 0.9267 - f1: 0.7760 - val_loss: 0.1748 - val_acc: 0.9295 - val_f1: 0.7884\n",
      "Epoch 3/10\n",
      "348472/348472 [==============================] - 163s 469us/step - loss: 0.1723 - acc: 0.9321 - f1: 0.7944 - val_loss: 0.1697 - val_acc: 0.9318 - val_f1: 0.7979\n",
      "Epoch 4/10\n",
      "348472/348472 [==============================] - 163s 468us/step - loss: 0.1617 - acc: 0.9361 - f1: 0.8073 - val_loss: 0.1665 - val_acc: 0.9336 - val_f1: 0.7988\n",
      "Epoch 5/10\n",
      "348472/348472 [==============================] - 163s 468us/step - loss: 0.1510 - acc: 0.9404 - f1: 0.8206 - val_loss: 0.1691 - val_acc: 0.9348 - val_f1: 0.8001\n",
      "Epoch 6/10\n",
      "348472/348472 [==============================] - 163s 468us/step - loss: 0.1398 - acc: 0.9449 - f1: 0.8352 - val_loss: 0.1697 - val_acc: 0.9331 - val_f1: 0.7963\n",
      "Epoch 7/10\n",
      "348472/348472 [==============================] - 163s 468us/step - loss: 0.1288 - acc: 0.9494 - f1: 0.8495 - val_loss: 0.1837 - val_acc: 0.9328 - val_f1: 0.7920\n",
      "Epoch 8/10\n",
      "348472/348472 [==============================] - 163s 468us/step - loss: 0.1164 - acc: 0.9549 - f1: 0.8659 - val_loss: 0.1929 - val_acc: 0.9308 - val_f1: 0.7951\n",
      "Epoch 9/10\n",
      "348472/348472 [==============================] - 163s 467us/step - loss: 0.1038 - acc: 0.9597 - f1: 0.8808 - val_loss: 0.2013 - val_acc: 0.9287 - val_f1: 0.7867\n",
      "Epoch 10/10\n",
      "348000/348472 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9647 - f1: 0.8957"
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=10,batch_size=500,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261225/261225 [==============================] - 37s 143us/step\n",
      "0.6442800227660785\n"
     ]
    }
   ],
   "source": [
    "preds= model.predict_classes(test_X,verbose=1,batch_size=1000)\n",
    "print(metrics.f1_score(test_Y, preds))\n",
    "# pred_cnn_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM+Attention  - Multiple embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 180)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 180, 300)     40041900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 180, 300)     40041900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 300)          542400      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 300)          542400      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 600)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          60100       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            101         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 81,228,801\n",
      "Trainable params: 1,145,001\n",
      "Non-trainable params: 80,083,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##model definition\n",
    "input_shape = (MAX_SEQUENCE_LENGTH,)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z1 = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix1],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)(model_input)\n",
    "z1 = Bidirectional(CuDNNLSTM(150, return_sequences=False), input_shape=(train_X.shape[1], EMBEDDING_DIM))(z1)\n",
    "\n",
    "z2 = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix2],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)(model_input)\n",
    "z2 = Bidirectional(CuDNNLSTM(150, return_sequences=False), input_shape=(train_X.shape[1], EMBEDDING_DIM))(z2)\n",
    "\n",
    "m = Concatenate()([z1,z2])\n",
    "m = Dropout(0.4)(m)\n",
    "m = Dense(100,activation='relu')(m)\n",
    "m = Dropout(0.4)(m)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(m)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",f1])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348472 samples, validate on 38720 samples\n",
      "Epoch 1/10\n",
      "348472/348472 [==============================] - 172s 492us/step - loss: 0.1251 - acc: 0.9512 - f1: 0.8549 - val_loss: 0.1791 - val_acc: 0.9315 - val_f1: 0.7925\n",
      "Epoch 2/10\n",
      "348472/348472 [==============================] - 178s 511us/step - loss: 0.1159 - acc: 0.9555 - f1: 0.8682 - val_loss: 0.1976 - val_acc: 0.9298 - val_f1: 0.7840\n",
      "Epoch 3/10\n",
      "348472/348472 [==============================] - 178s 510us/step - loss: 0.1067 - acc: 0.9592 - f1: 0.8790 - val_loss: 0.1977 - val_acc: 0.9292 - val_f1: 0.7868\n",
      "Epoch 4/10\n",
      "348472/348472 [==============================] - 181s 519us/step - loss: 0.0982 - acc: 0.9625 - f1: 0.8887 - val_loss: 0.2056 - val_acc: 0.9296 - val_f1: 0.7887\n",
      "Epoch 5/10\n",
      "348472/348472 [==============================] - 180s 517us/step - loss: 0.0875 - acc: 0.9667 - f1: 0.9015 - val_loss: 0.2267 - val_acc: 0.9292 - val_f1: 0.7865\n",
      "Epoch 6/10\n",
      "348472/348472 [==============================] - 177s 508us/step - loss: 0.0797 - acc: 0.9699 - f1: 0.9108 - val_loss: 0.2405 - val_acc: 0.9270 - val_f1: 0.7796\n",
      "Epoch 7/10\n",
      "348472/348472 [==============================] - 177s 508us/step - loss: 0.0717 - acc: 0.9729 - f1: 0.9199 - val_loss: 0.2634 - val_acc: 0.9254 - val_f1: 0.7683\n",
      "Epoch 8/10\n",
      "348472/348472 [==============================] - 176s 504us/step - loss: 0.0642 - acc: 0.9760 - f1: 0.9290 - val_loss: 0.2641 - val_acc: 0.9246 - val_f1: 0.7773\n",
      "Epoch 9/10\n",
      "348472/348472 [==============================] - 175s 503us/step - loss: 0.0571 - acc: 0.9788 - f1: 0.9372 - val_loss: 0.3034 - val_acc: 0.9198 - val_f1: 0.7650\n",
      "Epoch 10/10\n",
      "348472/348472 [==============================] - 177s 507us/step - loss: 0.0506 - acc: 0.9813 - f1: 0.9445 - val_loss: 0.3305 - val_acc: 0.9241 - val_f1: 0.7651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18ec651390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=10,batch_size=500,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261225/261225 [==============================] - 46s 177us/step\n",
      "0.6312801589554358\n"
     ]
    }
   ],
   "source": [
    "probs= model.predict(test_X,verbose=1,batch_size=500)\n",
    "preds = np.zeros(len(probs))\n",
    "\n",
    "preds[probs[:,0]>=0.5] =1 \n",
    "print(metrics.f1_score(test_Y, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold -0.2 is 0.5\n",
      "F1 score at threshold -0.15 is 0.5\n",
      "F1 score at threshold -0.1 is 0.5\n",
      "F1 score at threshold -0.05 is 0.5\n",
      "F1 score at threshold -0.0 is 0.5006551699569726\n",
      "F1 score at threshold 0.05 is 0.9538913433701808\n",
      "F1 score at threshold 0.1 is 0.9588053741119749\n",
      "F1 score at threshold 0.15 is 0.9608015640273705\n",
      "F1 score at threshold 0.2 is 0.9622097119018885\n",
      "F1 score at threshold 0.25 is 0.9630757144260041\n",
      "F1 score at threshold 0.3 is 0.9636065150425176\n",
      "F1 score at threshold 0.35 is 0.9639653505882643\n",
      "F1 score at threshold 0.4 is 0.9641520716588683\n",
      "F1 score at threshold 0.45 is 0.964307977164018\n",
      "F1 score at threshold 0.5 is 0.9644417258274791\n",
      "F1 score at threshold 0.55 is 0.9646491200940216\n",
      "F1 score at threshold 0.6 is 0.9645400611848054\n",
      "F1 score at threshold 0.65 is 0.964271098158368\n",
      "F1 score at threshold 0.7 is 0.9639306803434937\n",
      "F1 score at threshold 0.75 is 0.9634884906669319\n",
      "F1 score at threshold 0.8 is 0.9628457710203242\n",
      "F1 score at threshold 0.85 is 0.9619818616466829\n",
      "F1 score at threshold 0.9 is 0.9603125410867565\n",
      "F1 score at threshold 0.95 is 0.9561179094884624\n"
     ]
    }
   ],
   "source": [
    "for thresh in np.arange(-0.20, 1, 0.05):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(train_Y, (preds>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 9s 163us/step\n",
      "[7259]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(test_X,verbose=1)\n",
    "# print(metrics.f1_score(train_Y,preds))\n",
    "print(sum(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame()\n",
    "df_out[\"qid\"] = df_test['qid']\n",
    "df_out[\"predictions\"] = preds\n",
    "df_out.to_csv(\"sub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([81037], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 150, 300)          30870600  \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 150, 280)          495040    \n",
      "=================================================================\n",
      "Total params: 31,365,640\n",
      "Trainable params: 495,040\n",
      "Non-trainable params: 30,870,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 1)                 151       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_28 (RepeatVect (None, 150, 1)            0         \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=150, units=1)`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-62b9190e6e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# model.add(Merge([model1, model2], 'mul'))  # Multiply each element with corresponding weight a[i][j][k] * b[i][j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributedMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D, Flatten\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(10,(3,3)))\n",
    "model.add(Conv2D(10,(3,3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20))\n",
    "model.add(Dense(10))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# model.build()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf; print(tf.__version__)\n",
    "import keras; print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "letter_count = dict(zip(string.ascii_lowercase, [0]*26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{chr(i+96):i for i in range(1,27)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
