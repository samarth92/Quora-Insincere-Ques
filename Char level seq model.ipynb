{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import *\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n",
      "Train shape :  (242430, 3)\n",
      "qid              527aac2ce6f12f789fe5\n",
      "question_text                       \"\n",
      "target                              1\n",
      "Name: 420816, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./input/train.csv\")\n",
    "df_test = pd.read_csv(\"./input/test.csv\")\n",
    "print(\"Train shape : \",df_train.shape)\n",
    "print(\"Test shape : \",df_test.shape)\n",
    "\n",
    "df_train_pos = df_train[df_train['target']==1]\n",
    "df_train_neg = df_train[df_train['target']==0].sample(len(df_train_pos)*2,random_state=1)\n",
    "df_train = pd.concat([df_train_pos,df_train_neg])\n",
    "df_train = df_train.sample(frac=1,random_state=1)\n",
    "print(\"Train shape : \",df_train.shape)\n",
    "\n",
    "## fill up the missing values\n",
    "# train_X_raw = df_train[\"question_text\"].fillna(\"_##_\").values\n",
    "# test_X_raw = df_test[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "df_train.dropna(inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "\n",
    "char_dict = {chr(i+96):i for i in range(1,27)}\n",
    "char_dict[' '] = 0 \n",
    "vocab_size = len(char_dict)\n",
    "\n",
    "train_X_filt = []\n",
    "train_Y = []\n",
    "for i in range(len(df_train)):\n",
    "    s=[]\n",
    "    for char in df_train.iloc[i]['question_text'].lower():\n",
    "        if char in char_dict.keys():\n",
    "            s.append(char_dict[char])\n",
    "    if s!=[]:\n",
    "        train_X_filt.append(s)\n",
    "        train_Y.append(df_train.iloc[i]['target'])\n",
    "    else:\n",
    "        print(df_train.iloc[i])\n",
    "train_Y = np.asarray(train_Y)\n",
    "        \n",
    "test_X_filt = []\n",
    "for sent in df_test[\"question_text\"].values:\n",
    "    s=[]\n",
    "    for char in sent.lower():\n",
    "        if char in char_dict.keys():\n",
    "            s.append(char_dict[char])\n",
    "    test_X_filt.append(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is water a resource or not?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train.iloc[0]['question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.45604280016005\n",
      "25.0\n",
      "1.1047665728195455\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([6.34602931e-06, 0.00000000e+00, 0.00000000e+00, 1.58650733e-06,\n",
       "        0.00000000e+00, 1.58650733e-05, 2.53841172e-05, 2.92551951e-03,\n",
       "        9.34801847e-02, 2.88160499e-01]),\n",
       " array([ 0. ,  2.6,  5.2,  7.8, 10.4, 13. , 15.6, 18.2, 20.8, 23.4, 26. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = []\n",
    "for sent in train_X_filt:\n",
    "#     print(sent)\n",
    "    lengths.append(max(sent))\n",
    "    if max(sent)>50:\n",
    "        print(sent)\n",
    "print(np.mean(lengths))\n",
    "print(np.median(lengths))\n",
    "print(np.std(lengths))\n",
    "print(np.max(lengths))\n",
    "# print(len(lengths>200))\n",
    "plt.hist(lengths,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 140\n",
    "# Pad the sentences \n",
    "train_X_filt = pad_sequences(train_X_filt, maxlen=maxlen, padding=\"post\")\n",
    "test_X_filt = pad_sequences(test_X_filt, maxlen=maxlen, padding =\"post\")\n",
    "\n",
    "\n",
    "train_X = np.asarray([to_categorical(x, num_classes=vocab_size) for x in train_X_filt])\n",
    "# test_X_filt = np.asarray([to_categorical(x, num_classes=vocab_size) for x in test_X_filt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242429, 140, 27)\n",
      "(242429,)\n"
     ]
    }
   ],
   "source": [
    "# print(train_X_raw[0])\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "# print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 140, 150)          62400     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 21000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 21001     \n",
      "=================================================================\n",
      "Total params: 83,401\n",
      "Trainable params: 83,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##model definition\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(CuDNNLSTM(75, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Bidirectional(CuDNNLSTM(75, return_sequences=True,), input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 193943 samples, validate on 48486 samples\n",
      "Epoch 1/100\n",
      "193943/193943 [==============================] - 18s 93us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 2/100\n",
      "193943/193943 [==============================] - 18s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 3/100\n",
      "193943/193943 [==============================] - 18s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 4/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 5/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 6/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 7/100\n",
      "193943/193943 [==============================] - 18s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 8/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 9/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 10/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 11/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 12/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 13/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 14/100\n",
      "193943/193943 [==============================] - 18s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 15/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 16/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 17/100\n",
      "193943/193943 [==============================] - 18s 91us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 18/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 19/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 20/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 21/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 22/100\n",
      "193943/193943 [==============================] - 18s 91us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 23/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 24/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 25/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 26/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 27/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 28/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 29/100\n",
      "193943/193943 [==============================] - 18s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 30/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 31/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 32/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 33/100\n",
      "193943/193943 [==============================] - 18s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 34/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 35/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 36/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 37/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 38/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 39/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 40/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 41/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 42/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 43/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 44/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 45/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 46/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 47/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 48/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 49/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 50/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 51/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 52/100\n",
      "193943/193943 [==============================] - 17s 87us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 53/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 54/100\n",
      "193943/193943 [==============================] - 17s 87us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 55/100\n",
      "193943/193943 [==============================] - 17s 87us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 56/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 58/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 59/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 60/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 61/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 62/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 63/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 64/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 65/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 66/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 67/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 68/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 69/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 70/100\n",
      "193943/193943 [==============================] - 17s 87us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 71/100\n",
      "193943/193943 [==============================] - 17s 87us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 72/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 73/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 74/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 75/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 76/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 77/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 78/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 79/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 80/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 81/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 82/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 83/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 84/100\n",
      "193943/193943 [==============================] - 17s 86us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 85/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 86/100\n",
      "193943/193943 [==============================] - 17s 90us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 87/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 88/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 89/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 90/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 91/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 92/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 93/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 94/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 95/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 96/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 97/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 98/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 99/100\n",
      "193943/193943 [==============================] - 17s 89us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n",
      "Epoch 100/100\n",
      "193943/193943 [==============================] - 17s 88us/step - loss: 10.6159 - acc: 0.3341 - val_loss: 10.6779 - val_acc: 0.3302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc200167da0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=100,batch_size=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D, Flatten\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(10,(3,3)))\n",
    "model.add(Conv2D(10,(3,3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20))\n",
    "model.add(Dense(10))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# model.build()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.__version__)\n",
    "import keras; print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "letter_count = dict(zip(string.ascii_lowercase, [0]*26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{chr(i+96):i for i in range(1,27)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the sentences\n",
    "# tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, char_level=True)\n",
    "# tokenizer.fit_on_texts(list(train_X_raw))\n",
    "\n",
    "# train_X = tokenizer.texts_to_sequences(train_X_raw)\n",
    "# test_X = tokenizer.texts_to_sequences(test_X_raw)\n",
    "\n",
    "# for sent in train_X_raw:\n",
    "#     s=[]\n",
    "#     for char in sent.lower():\n",
    "#         if char in char_dict.keys():\n",
    "#             s.append(char_dict[char])\n",
    "#     if s!=[]:\n",
    "#         train_X_filt.append(s)\n",
    "#     else:\n",
    "#         print(sent)\n",
    "\n",
    "# ## Get the target values\n",
    "# # train_Y = df_train['target'].values\n",
    "\n",
    "# test_X_filt = []\n",
    "# for sent in test_X_raw:\n",
    "#     s=[]\n",
    "#     for char in sent.lower():\n",
    "#         if char in char_dict.keys():\n",
    "#             s.append(char_dict[char])\n",
    "#     test_X_filt.append(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
